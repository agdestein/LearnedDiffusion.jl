{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Operator inference\n",
    "\n",
    "In this example, we will identify an unknown operator describing a physical system by\n",
    "combining tools from scientific computing (PDEs, discretizations) and machine learning\n",
    "(minimizing prediction errors, backpropagation).\n",
    "\n",
    "For this, the [Julia](https://julialang.org/) language is a natural choice,\n",
    "in this example because of automatic differentiation (AD) of native code."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Julia bootstrap block (for Google Colab)\n",
    "Source: https://colab.research.google.com/drive/1_4Yz3FKO5_uuYvamEfHqwtFT9WpCuSbm\n",
    "\n",
    "This should be run for the first time to install Julia kernel, and then refresh this page (e.g., Ctrl-R)\n",
    "so that colab will redirect to the installed Julia kernel\n",
    "and then doing your own work"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# 1. install latest Julia using jill.py\n",
    "#    tip: one can install specific Julia version using e.g., `jill install 1.7`\n",
    "!pip install jill && jill install --upstream Official --confirm\n",
    "\n",
    "# 2. install IJulia kernel\n",
    "! julia -e 'using Pkg; pkg\"add IJulia\"; using IJulia; installkernel(\"Julia\")'\n",
    "\n",
    "# 3. hot-fix patch to strip the version suffix of the installed kernel so\n",
    "# that this notebook kernelspec is version agnostic\n",
    "!jupyter kernelspec install $(jupyter kernelspec list | grep julia | tr -s ' ' | cut -d' ' -f3) --replace --name julia"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Pkg; Pkg.add([\"OrdinaryDiffEq\", \"DiffEqFlux\", \"Plots\"])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by loading some packages."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using LinearAlgebra\n",
    "using SparseArrays\n",
    "using OrdinaryDiffEq\n",
    "using DiffEqFlux\n",
    "using Plots"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That may have taken some time. Julia is a compiled language, and she won't\n",
    "hesitate to compile any code she can put her eyes on, even just to get a\n",
    "small specialization improvement. Once heavy simulations are launched,\n",
    "however, this efficiency will be worth your patience. The first time you run\n",
    "some of the cells below it may also take some time, as the functions, say,\n",
    "`heatmap`, specializes on your input types. Once a function is compiled\n",
    "however, it should stay so for the remainder of the session."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the problems will consist of inferring various matrices, we define a small helper\n",
    "function for visualizing our results:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(A; kwargs...) = heatmap(\n",
    "    reverse(A; dims = 1);\n",
    "    aspect_ratio = :equal,\n",
    "    xlims = (1 / 2, size(A, 2) + 1 / 2),\n",
    "    ylims = (1 / 2, size(A, 1) + 1 / 2),\n",
    "    # xticks = nothing,\n",
    "    # yticks = nothing,\n",
    "    kwargs...,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also provide a specialized method for sparse matrices, which must be densified before\n",
    "plotting."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(A::AbstractSparseMatrix; kwargs...) = plotmat(Matrix(A); kwargs...)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem statement\n",
    "\n",
    "Consider a linear ordinary differential equation (ODE) parameterized by some operator\n",
    "$\\mathbf{A}$:\n",
    "\n",
    "$$\\frac{\\mathrm{d} \\mathbf{u}}{\\mathrm{d} t} = \\mathbf{A} \\mathbf{u}, \\quad \\mathbf{u}(0)\n",
    "= \\mathbf{u}_0$$\n",
    "\n",
    "where $\\mathbf{u}_0$ are some initial conditions. To solve this system, we will use the\n",
    "[OrdinaryDiffEq](https://github.com/SciML/OrdinaryDiffEq.jl) package. It provides\n",
    "differentiable ODE solvers for problems defined by a parametrized ODE function $f$\n",
    "defining the right hand side of the ODE for a given state $\\mathbf{u}$, time $t$ and\n",
    "parameters $p$ (in our case: $p = \\mathbf{A}$), i.e. $\\frac{\\mathrm{d}\n",
    "\\mathbf{u}}{\\mathrm{d} t} = f(\\mathbf{u}, p, t)$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f(u, A, t) = A * u"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For convenience, we will define a solver function $\\mathbf{S}: (\\mathbf{A}, \\mathbf{u}_0,\n",
    "t) \\mapsto \\mathbf{u}(t)$, where $\\mathbf{u}$ is the solution to the above system for a\n",
    "given operator $\\mathbf{A}$ and initial conditions $\\mathbf{u}_0$. The method `Tsit5`\n",
    "is a fourth order Runge Kutta method."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "?Tsit5"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function S(A, u₀, t)\n",
    "    problem = ODEProblem(ODEFunction(f), u₀, (0.0, t[end]), A)\n",
    "    solve(problem, Tsit5(); saveat = t)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider now the diffusion equation\n",
    "\n",
    "$$\\frac{\\partial u}{\\partial t} = \\kappa \\frac{\\partial^2 u}{\\partial x^2}, \\quad x \\in\n",
    "\\Omega = [a, b]$$\n",
    "\n",
    "with diffusivity $\\kappa > 0$, homogeneous Dirichlet boundary conditions $u(a, t) = u(b,\n",
    "t) = 0$, and initial conditions $u(x, 0) = u_0(x)$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "κ = 0.005\n",
    "a = 0.0\n",
    "b = 1.0\n",
    "L = b - a"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The domain $\\Omega$ may be discretized using a uniform grid $\\mathbf{x} = (x_n)_{0 \\leq n\n",
    "\\leq N}$ of $N + 1$ equidistant points. We will also make a refined grid `xfine` for\n",
    "plotting."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xfine = LinRange(a, b, 1001)\n",
    "N = 50\n",
    "x = LinRange(a, b, N + 1)\n",
    "Δx = L / N"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the above grid, the diffusion operator $\\frac{\\partial^2}{\\partial x^2}$ with constant\n",
    "Dirichlet boundary conditions may be approximated using the matrix\n",
    "\n",
    "$$\\mathbf{D} = \\frac{1}{\\Delta x^2} \\begin{pmatrix}\n",
    "    0 &  \\dots &  \\dots &  \\dots & 0 \\\\\n",
    "    1 &     -2 &      1 &        &   \\\\\n",
    "      & \\ddots & \\ddots & \\ddots &   \\\\\n",
    "      &        &      1 &     -2 & 1 \\\\\n",
    "    0 &  \\dots &  \\dots &  \\dots & 0 \\\\\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "This approximation is second order accurate:\n",
    "\n",
    "$$(\\mathbf{D} \\mathbf{u})_n = \\frac{\\partial^2 u}{\\partial x^2}(x_n) + \\mathcal{O}(\\Delta\n",
    "x^2).$$"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "D = 1 / Δx^2 * spdiagm(-1 => fill(1.0, N), 0 => fill(-2.0, N + 1), 1 => fill(1.0, N))\n",
    "D[1, :] .= 0 # Do not change first value\n",
    "D[end, :] .= 0 # Do not change last value\n",
    "plotmat(D)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The semi-discrete solution $t \\mapsto \\mathbf{u}(t) = (u(x_n, t))_n \\in \\mathbb{R}^{N +\n",
    "1}$ may be approximated using the solver $\\mathbf{S}$:\n",
    "\n",
    "$$\\mathbf{u}(t) \\approx \\mathbf{S}(\\kappa \\mathbf{D}, u_0(\\mathbf{x}), t),$$\n",
    "\n",
    "i.e. we set $\\mathbf{A} = \\kappa \\mathbf{D}$ in the ODE."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "A_ref = κ * D\n",
    "plotmat(A_ref)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The diffusion operator $\\frac{\\partial^2}{\\partial x^2}$ with homogeneous boundary\n",
    "conditions on $\\Omega$ admit an eigenfunction basis $(X_k)_{k \\in \\mathbb{N}^*}$ with\n",
    "\n",
    "$$X_k(x) = \\sqrt{\\frac{2}{L}} \\sin \\left( \\frac{\\pi k x}{L} \\right).$$\n",
    "\n",
    "The associated eigenvalues are given by $\\lambda_k = -\\frac{\\pi^2 k^2}{L^2}$. Since these\n",
    "functions form a basis of $L^2(\\Omega)$, all solutions to the diffusion equation with\n",
    "homogeneous boundary conditions may be written on the form\n",
    "\n",
    "$$u(x, t) = \\sum_{k \\in \\mathbb{N}^*} c_k \\exp \\left( - \\kappa \\frac{\\pi^2 k^2}{L^2}\n",
    "t \\right) X_k(x),$$\n",
    "\n",
    "where the coefficients $(c_k)_{k \\in \\mathbb{N}^*}$ are determined by projecting the\n",
    "initial conditions onto the (orthogonal) eigenfunction basis:\n",
    "\n",
    "$$c_k = \\int_\\Omega u_0(x) X_k(x) \\, \\mathrm{d} x.$$\n",
    "\n",
    "In particular, we may use this formulation to generate exact solutions to the diffusion\n",
    "equation, by providing arbitrary coefficients. These solutions may used as training data\n",
    "for identifying the \"unknown\" discrete diffusion operator $\\mathbf{A}$ (of which a very\n",
    "promising second-order accurate candidate is given by $\\mathbf{A}_\\text{ref} = \\kappa\n",
    "\\mathbf{D}$)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X(k, x) = √(2 / L) * sin(π * k * (x - a) / L)\n",
    "p = plot()\n",
    "for k = 1:5\n",
    "    plot!(p, xfine, X.(k, xfine); label = \"k = $k\")\n",
    "end\n",
    "p"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_solution(c, k)\n",
    "    u(x, t) = sum(c * exp(-κ * (π * k / L)^2 * t) * X(k, x) for (c, k) in zip(c, k))\n",
    "    ∂u∂t(x, t) =\n",
    "        -sum(\n",
    "            c * κ * (π * k / L)^2 * exp(-κ * (π * k / L)^2 * t) * X(k, x) for\n",
    "            (c, k) ∈ zip(c, k)\n",
    "        )\n",
    "    u, ∂u∂t\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A particular solution is given below, containing three different frequencies. We may use\n",
    "this solution to test our solver, from now on referred to as the *full order model* (FOM)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "k = [3, 7, 10]\n",
    "c = [0.7, 0.3, 0.4]\n",
    "u, ∂u∂t = create_solution(c, k)\n",
    "p = plot();\n",
    "tplot = LinRange(0.0, 1.0, 5)\n",
    "sol = S(A_ref, u.(x, 0.0), tplot)\n",
    "for (i, t) ∈ enumerate(tplot)\n",
    "    plot!(p, xfine, u.(xfine, t); label = \"t = $t\", color = i) # Exact\n",
    "    scatter!(p, x, sol[i]; label = nothing, markeralpha = 0.5, color = i) # FOM\n",
    "end\n",
    "p"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning the operator intrusively\n",
    "\n",
    "Before inferring the unknown operator, we need som \"training\" data to compare\n",
    "with. This will consist of snapshots of different initial conditions diffused\n",
    "for different durations. We will sample normally distributed random\n",
    "coefficients decaying with the frequency $k$, and put the results in a\n",
    "snapshot tensor of size $N \\times n_\\text{sample} \\times n_t$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tsnap = LinRange(0.0, 1.0, 51)[2:end]\n",
    "nsample = 1000\n",
    "K = 50\n",
    "k = 1:K\n",
    "c = [randn(K) ./ k for _ = 1:nsample]\n",
    "solutions = [create_solution(c, k) for c ∈ c]\n",
    "u = [s[1] for s ∈ solutions]\n",
    "∂u∂t = [s[2] for s ∈ solutions]\n",
    "init = [u(x, 0.0) for x ∈ x, u ∈ u]\n",
    "train = [u(x, t) for x ∈ x, u ∈ u, t ∈ tsnap]\n",
    "∂train∂t = [∂u∂t(x, t) for x ∈ x, ∂u∂t ∈ ∂u∂t, t ∈ tsnap]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need an instantaneous performance metric (loss/cost/objective\n",
    "function). This function should compare our predictions with a snapshots of\n",
    "the exact solutions. Here we will use a simple $L^2$-distance (mean squared\n",
    "error). Note that the `ODESolution` object behaves like an array of size $N\n",
    "\\times n_\\text{sample} \\times n_t$, meaning that we solve for all the\n",
    "different initial conditions at the same time."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss(A, u₀, uₜ, t) = sum(abs2, S(A, u₀, t) - uₜ) / prod(size(uₜ))\n",
    "loss(A) = loss(A, init, train, tsnap)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an initial guess for the \"unknown\" operator $\\mathbf{A}$ we will simply\n",
    "use an empty matrix."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "A = zeros(N + 1, N + 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We may also visualize the performance of our operator."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function ploterr(A, u, tplot = LinRange(0.0, 1.0, 5))\n",
    "    sol = S(A, u.(x, 0.0), tplot)\n",
    "    p = plot()\n",
    "    for (i, t) ∈ enumerate(tplot)\n",
    "        plot!(p, xfine, u.(xfine, t); color = i, label = nothing)\n",
    "        scatter!(p, x, sol[i]; label = \"t = $t\", color = i, markeralpha = 0.5)\n",
    "    end\n",
    "    p\n",
    "end\n",
    "ploterr(A_ref, u[6])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A callback function is called after every iteration of gradient descent, allowing us to\n",
    "check the performance of our operator in real time during training. The return value\n",
    "`false` simply stops the function from stopping the iterations. We can already check how\n",
    "our initial guess for $\\mathbf{A}$ performs.\n",
    "\n",
    "You may choose any of the callbacks below. For Jupyter notebooks, the two\n",
    "first options will print a plot after every iteration, which will quickly become very\n",
    "verbose. You may want to add a counter, plotting only every 10th itereation or so, or use\n",
    "the less verbose third callback option."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function callback(A, loss)\n",
    "    println(loss)\n",
    "    flush(stdout)\n",
    "    display(\n",
    "        plot(\n",
    "            plotmat(A; title = \"Predict\"),\n",
    "            plotmat(A_ref; title = \"Reference\");\n",
    "            layout = (1, 2),\n",
    "        ),\n",
    "    )\n",
    "    false\n",
    "end\n",
    "callback(A, loss(A))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function callback(A, loss)\n",
    "    println(loss)\n",
    "    flush(stdout)\n",
    "    display(ploterr(A, u[1], LinRange(0.0, 2.0, 5)))\n",
    "    false\n",
    "end\n",
    "callback(A, loss(A))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "callback(A, loss) = (println(loss);\n",
    "flush(stdout);\n",
    "false)\n",
    "callback(A, loss(A))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The intrusive training consists of improving the operator through gradient\n",
    "descent applied to the loss function. The optimizer\n",
    "[`ADAM`](https://arxiv.org/abs/1412.6980) performs a first order gradient\n",
    "descent, but with some sophisiticated momentum terms exploiting the\n",
    "stochasticity of the loss function. For larger problems we could could use a\n",
    "subset of the different solutions $u$, time steps $t$ and spatial points $x$\n",
    "at every evaluation of `loss`, but for now we will just use the entire\n",
    "dataset."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "result = DiffEqFlux.sciml_train(loss, A, ADAM(0.01); cb = callback, maxiters = 1000)\n",
    "Afit = result.u\n",
    "plotmat(Afit)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ploterr(Afit, u[4])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that at no point did we explicitly specify the gradient of `loss`, `S`\n",
    "or even `f` with respect to the matrix `A`. Yet still we performed a gradient\n",
    "descent. Since the entire computational graph is composed of pure Julia code,\n",
    "automatic differentiation engines, in this particular case\n",
    "[Zygote](https://github.com/FluxML/Zygote.jl), can use the chain rule to\n",
    "compute gradients. We may access this gradient explicitly. Let us check for\n",
    "the initial guess first:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "∂L∂A = first(Zygote.gradient(loss, A))\n",
    "plotmat(∂L∂A)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and for the final result:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "∂L∂Afit = first(Zygote.gradient(loss, Afit))\n",
    "plotmat(∂L∂Afit)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The gradient of the fitted operator is indeed much closer to zero:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "norm(∂L∂A)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "norm(∂L∂Afit)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note also that the optimizer used here is [ADAM](https://arxiv.org/abs/1412.6980), which\n",
    "is typically used to train neural networks. In fact, our ODE solver is not so different\n",
    "from a neural network. Here we used a 4/5 Runge Kutta solver, but consider for\n",
    "illustrative purposes a simple forward Euler scheme $\\frac{\\mathbf{u}^{n+1} -\n",
    "\\mathbf{u}^n}{\\Delta t} = \\mathbf{A} \\mathbf{u}^n$. It satisfies the definition of a\n",
    "\"vanilla\" neural network:\n",
    "\n",
    "| Neural network | ODE |\n",
    "| :------------: | :-: |\n",
    "| $$\\mathbf{x} \\to \\left[ \\operatorname{NN}_\\theta \\right] \\to \\mathbf{y}$$ | $$\\mathbf{u}_0\n",
    "\\to \\left[ \\frac{\\mathrm{d} \\mathbf{u}}{\\mathrm{d} t} = \\mathbf{A} \\mathbf{u} \\right] \\to\n",
    "\\mathbf{u}(T)$$ |\n",
    "| $$\\mathbf{h}_0 = \\mathbf{x}$$ | $$\\mathbf{u}_0 = \\mathbf{u}_0$$ |\n",
    "| $$\\mathbf{h}_{k + 1} = \\sigma_k(\\mathbf{W}_k \\mathbf{h}_k +\n",
    "\\mathbf{b}_k)$$ | $$\\mathbf{u}_{k + 1} = (\\mathbf{I} + \\Delta t_k \\mathbf{A})\n",
    "\\mathbf{u}_k$$ |\n",
    "| $$\\mathbf{y} = \\mathbf{h}_K$$ | $$\\mathbf{u}(T) = \\mathbf{u}_K$$ |\n",
    "| $$\\underset{\\theta}{\\min} \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim \\mathcal P} \\|\n",
    "\\mathbf{y}_\\theta(\\mathbf{x}) - \\mathbf{y} \\|^2$$ | $$\\underset{\\mathbf{A}}{\\min}\n",
    "\\mathbb{E}_{\\mathbf{u} \\sim \\mathcal{U}} \\| \\mathbf{u}_\\mathbf{A}(T) - \\mathbf{u}(T)\n",
    "\\|^2$$ |\n",
    "\n",
    "\n",
    "\n",
    "### Adding a regularization\n",
    "\n",
    "We may also try out different loss functions. Since the resulting matrix from the above\n",
    "fit is quite dense, we could maybe enforce sparsity by adding penalization jump for going\n",
    "from zero to non-zero coefficients."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ℓ₁loss(A) = loss(A) + 1e-2 * sum(abs, A) / prod(size(A))\n",
    "result = DiffEqFlux.sciml_train(ℓ₁loss, A, ADAM(0.01); cb = callback, maxiters = 1000)\n",
    "A_ℓ₁ = result.u\n",
    "plotmat(A_ℓ₁)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ℓ₂loss(A) = loss(A) + 1e-2 * sum(abs2, A) / prod(size(A))\n",
    "result = DiffEqFlux.sciml_train(ℓ₂loss, A, ADAM(0.01); cb = callback, maxiters = 1000)\n",
    "A_ℓ₂ = result.u\n",
    "plotmat(A_ℓ₂)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ploterr(A_ℓ₁, u[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ploterr(A_ℓ₂, u[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us compare the resulting matrices:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(\n",
    "    plotmat(Afit; title = \"No reg\"),\n",
    "    plotmat(A_ℓ₁; title = \"L¹\"),\n",
    "    plotmat(A_ℓ₂; title = \"L²\");\n",
    "    layout = (1, 3),\n",
    "    size = (900, 300),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indeed, the $L^1$-regularized matrix is sparser."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Non-intrusive operator inference\n",
    "\n",
    "It is also possible to infer the operator $\\mathbf{A}$ without ever computing the gradient\n",
    "of $\\mathbf{S}$, using snapshot matrices only. Consider the above defined snapshot matrix\n",
    "$\\mathbf{U}$ as well as its left hand side equivalent $\\dot{\\mathbf{U}}$ (containing\n",
    "snapshots of the time derivatives $\\frac{\\mathrm{d} \\mathbf{u}}{\\mathrm{d} t}(t_k)$). The\n",
    "operator that best satisfies the original ODE at the snapshots of interest should be the\n",
    "solution to the following minimization problem:\n",
    "\n",
    "$$\\underset{\\mathbf{A} \\in \\mathbb{R}^{N \\times N}}{\\min} \\ell(\\mathbf{A}),$$\n",
    "\n",
    "where $\\ell$ is some performance metric, typically consisting of a data fitting term\n",
    "$\\ell_\\text{data}(\\mathbf{A}) = \\| \\mathbf{A} \\mathbf{U} - \\dot{\\mathbf{U}} \\|_F^2$, where\n",
    "$\\| \\mathbf{X} \\|_F = \\sqrt{\\sum_{i j} X_{i j}^2}$ is the Frobenius norm (we could use any\n",
    "discrete norm here). We would also like to add a regularization term $\\ell_\\text{reg}$ to\n",
    "enforce some expected behavior on the operator. Since the first and last component of the\n",
    "solution vector $\\mathbf{u}$ are zero (because of the boundary conditions), the first and\n",
    "last columns of $\\mathbf{A}$ do not affect the value of $\\ell_\\text{data}$. An\n",
    "$L^2$-regularization $\\ell_\\text{reg}(\\mathbf{A}) = \\lambda \\| \\mathbf{A} \\|_F^2$ simply\n",
    "incites these two columns to take the value zero.\n",
    "\n",
    "The solution in the case of an $L^2$-regularization is given by\n",
    "\n",
    "$$\\mathbf{A} = \\underset{\\mathbf{A} \\in \\mathbb{R}^{N \\times N}}{\\operatorname{argmin}} \\|\n",
    "\\mathbf{A} \\mathbf{U} - \\dot{\\mathbf{U}} \\|_F^2 + \\lambda \\| \\mathbf{A} \\|_F^2 =\n",
    "\\dot{\\mathbf{U}} \\mathbf{U}^\\mathsf{T} (\\mathbf{U} \\mathbf{U}^\\mathsf{T} + \\lambda\n",
    "\\mathbf{I})^{-1}.$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Information from one single solution may not fully describe the dynamics of other\n",
    "solutions, as they may contain different frequencies. We may build an augmented snapshot\n",
    "matrix by concatenating snapshot matrices for many different initial conditions. Here we\n",
    "will use the exact solutions, but we could also have used the approximations as above."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "U = reshape(train, N + 1, :)\n",
    "∂U∂t = reshape(∂train∂t, N + 1, :)\n",
    "\n",
    "plotmat(reshape(permutedims(train, (1, 3, 2)), N + 1, :); aspect_ratio = :none)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We may zoom in on the first five solutions;"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(\n",
    "    reshape(permutedims(train, (1, 3, 2)), N + 1, :)[:, 1:(5 * length(tsnap))];\n",
    "    aspect_ratio = :none,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "A_ls = ∂U∂t * U' / (U * U' + 1e-8I)\n",
    "plotmat(A_ls)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ploterr(A_ls, u[7], LinRange(0.0, 10.0, 5))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proper orthogonal decomposition (POD)\n",
    "\n",
    "Above we learned the discrete diffusion operator in the canonical basis of\n",
    "$\\mathbb{R}^N$. Another useful basis is obtained from a *proper orthogonal\n",
    "decomposition* (POD). It is determined from snapshot data of the solution at\n",
    "different time steps (`tsnap`) and possibly different initial conditions.\n",
    "Truncating this basis at a level $P \\ll N$ will yield *the* basis of size $P$\n",
    "with the smallest error energy for the training data (among all possibile\n",
    "bases spanning $P$-dimensional subspaces of $L^2(\\Omega)$)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The POD basis is simply just a collection of left singular vectors of our snapshot matrix\n",
    "$\\mathbf{U}$. We will keep the $P$ first basis functions (they are the most important, as\n",
    "`svd` orders them by decreasing singular value). The basis functions will be stored as\n",
    "columns in the matrix $\\mathbf{\\Phi} \\in \\mathbb{R}^{N \\times P}$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "P = 20\n",
    "decomp = svd(U)\n",
    "Φ = decomp.U[:, 1:P]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We may plot some POD modes."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "p = plot();\n",
    "i = 0\n",
    "for k ∈ [1, 3, 7]\n",
    "    i += 1\n",
    "    plot!(p, x, Φ[:, k]; label = \"Mode $k\", color = i)\n",
    "    # plot!(p, x, X.(k, x); label = \"k = $k\", color = i)\n",
    "    # plot!(p, x, XK[:, k]; label = nothing, linestyle = :dash, color = i)\n",
    "end\n",
    "p"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We may check the orthogonality by computing the inner product between each basis function\n",
    "pair."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(Φ'Φ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The matrix $\\mathbf{Φ} \\mathbf{Φ}^\\mathsf{T}$ can be considered to be a so called\n",
    "\"autoencoder\", with \"encoder\" $\\mathbf{Φ}^\\mathsf{T}$ and \"decoder\" $\\mathbf{Φ}$. The\n",
    "autoencoder should be closer to identity when keeping more modes, i.e. we may be tempted\n",
    "to write something like $\\mathbf{Φ} \\mathbf{Φ}^\\mathsf{T} \\underset{P \\to N}{\\to}\n",
    "\\mathbf{I}$ (by abuse of mathematical notation)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(Φ * Φ')"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Projecting the full order model onto the POD basis yields the reduced order model\n",
    "\n",
    "$$\\frac{\\mathrm{d} \\tilde{\\mathbf{u}}}{\\mathrm{d} t} = \\tilde{\\mathbf{A}}\n",
    "\\tilde{\\mathbf{u}},$$\n",
    "\n",
    "where $\\tilde{\\mathbf{u}}$ are the coordinates of the ROM solution in the POD basis and\n",
    "$\\tilde{\\mathbf{A}} = \\mathbf{\\Phi}^\\mathsf{T} \\mathbf{A} \\mathbf{\\Phi} \\in \\mathbb{R}^{P\n",
    "\\times P}$ is the reduced order operator. Later, we will try to infer this operator\n",
    "directly from data. Note that the ROM solution is given by $\\mathbf{u}_\\text{ROM} =\n",
    "\\mathbf{\\Phi} \\tilde{\\mathbf{u}}$.\n",
    "\n",
    "Note also that the FOM and ROM have the same form, just different sizes (and \"tilde\"s\n",
    "appearing everywhere). The ROM solution may thus simply be computed by\n",
    "\n",
    "$$\\mathbf{u}_\\text{ROM}(t) = \\mathbf{\\Phi} \\mathbf{S}(\\tilde{\\mathbf{A}},\n",
    "\\mathbf{\\Phi}^\\mathsf{T} \\mathbf{u}_0, t).$$"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "S_POD(A_pod, u₀, t) = Φ * S(A_pod, Φ'u₀, t)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us compare the solution of the ROM and FOM:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tplot = LinRange(0.0, 1.0, 5)\n",
    "sample = 3\n",
    "sol = S(A_ref, u[sample].(x, 0.0), tplot)\n",
    "sol_pod = S_POD(Φ' * A_ref * Φ, u[sample].(x, 0.0), tplot)\n",
    "p = plot();\n",
    "for (i, t) ∈ enumerate(tplot)\n",
    "    # scatter!(p, x, u.(x, t); label = nothing, markeralpha = 0.5, color = i) # Exact\n",
    "    plot!(p, x, sol[i]; label = \"t = $t\", color = i) # FOM\n",
    "    scatter!(p, x, sol_pod[:, i]; label = nothing, markeralpha = 0.5, color = i) # ROM\n",
    "end\n",
    "p"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For ten modes, we cannot visually see the difference (at least not on our training data).\n",
    "Try using fewer modes and see what happens!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning the operator in the POD basis\n",
    "\n",
    "Similarly to the full order model case, we may fit the POD operator\n",
    "$\\tilde{A}$ using intrusive and non-intrusive approaches."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Non-intrusive approach"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "V = Φ'U\n",
    "∂V∂t = Φ'∂U∂t\n",
    "\n",
    "A_POD_ls = ∂V∂t * V' / (V * V' + 1e-8I)\n",
    "plotmat(A_POD_ls)\n",
    "plotmat(Φ * A_POD_ls * Φ')\n",
    "ploterr(Φ * A_POD_ls * Φ', u[7], LinRange(0.0, 1.0, 5))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Intrusive approach\n",
    "\n",
    "First we need to create snapshot tensors of the POD solutions/observations:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "init_POD = Φ'init\n",
    "train_POD = zeros(P, nsample, length(tsnap))\n",
    "for i ∈ eachindex(tsnap)\n",
    "    train_POD[:, :, i] = Φ' * train[:, :, i]\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We may also reuse the loss function and ODE solver from the full order case\n",
    "(only the sizes change)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss_POD(A_POD) = loss(A_POD, init_POD, train_POD, tsnap)\n",
    "\n",
    "result = DiffEqFlux.sciml_train(\n",
    "    loss_POD,\n",
    "    zeros(P, P),\n",
    "    ADAM(0.01);\n",
    "    cb = (A, l) -> callback(Φ * A * Φ', l),\n",
    "    maxiters = 1000,\n",
    ")\n",
    "\n",
    "A_POD_fit = result.u"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(A_POD_fit)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(Φ * A_POD_fit * Φ')"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ploterr(Φ * A_POD_fit * Φ', u[7], LinRange(0.0, 1.0, 5))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning the operator in the eigenfunction basis\n",
    "\n",
    "In the continuous eigenfunction basis $(X_k)_k$, the continuous diffusion\n",
    "operator is an infinite diagonal matrix. Projecting the solution on to a\n",
    "truncated eigenfunction basis would thus result in a finite diagonal matrix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Non-intrusive approach"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "K_eig = 20\n",
    "XK = √Δx .* X.((1:K_eig)', x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(XK'XK)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(XK * XK')"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "W = XK'U\n",
    "∂W∂t = XK'∂U∂t\n",
    "\n",
    "A_eig_ls = ∂W∂t * W' / (W * W' + 1e-8I)\n",
    "plotmat(A_eig_ls)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(XK * A_eig_ls * XK')"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ploterr(XK * A_eig_ls * XK', u[7], LinRange(0.0, 1.0, 5))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Intrusive approach"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "init_eig = XK'init\n",
    "train_eig = zeros(P, nsample, length(tsnap))\n",
    "for i ∈ eachindex(tsnap)\n",
    "    train_eig[:, :, i] = XK' * train[:, :, i]\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss_eig(A) = loss(A, init_eig, train_eig, tsnap)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "result = DiffEqFlux.sciml_train(\n",
    "    loss_eig,\n",
    "    zeros(K_eig, K_eig),\n",
    "    ADAM(0.01);\n",
    "    cb = (A, l) -> callback(XK * A * XK', l),\n",
    "    maxiters = 1000,\n",
    ")\n",
    "A_eig_fit = result.u"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(A_eig_fit)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plotmat(XK * A_eig_fit * XK')"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ploterr(XK * A_eig_fit * XK', u[7], LinRange(0.0, 1.0, 5))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "In this example, we considered a uniformly discretized diffusion equation\n",
    "with homogeneous Dirichlet boundary conditions for three discrete functional\n",
    "bases:\n",
    "\n",
    "* the canonical basis of $\\mathbb{R}^N$: $\\mathbf{I} = (\\mathbf{e}_n)_n \\in\n",
    "  \\mathbb{R}^{N \\times N}$\n",
    "* the eigenfunction basis of $\\frac{\\partial^2}{\\partial x^2}$: $\\mathbf{X} =\n",
    "  (X_k(\\mathbf{x}))_k \\in \\mathbb{R}^{N \\times K}$, $K \\ll N$\n",
    "* the POD basis $\\mathbf{\\Phi} = (\\mathbf{\\phi}_p)_p \\in \\mathbb{R}^{N \\times\n",
    "  P}$, $P \\ll N$\n",
    "\n",
    "where the diffusion operator $\\frac{\\partial^2}{\\partial x^2}$ was\n",
    "represented by $\\mathbf{A}$, $\\bar{\\mathbf{A}}$, and $\\tilde{\\mathbf{A}}$\n",
    "respectively. These three operators, of respective sizes $N \\times N$, $K\n",
    "\\times K$, and $P \\times P$, were then inferred using two methods:\n",
    "\n",
    "* intrusive inference, where the ODE-solver $\\mathbf{S}$ needs to be available and\n",
    "  differentiable, and the operator is trained using gradient descent;\n",
    "* non-intrusive inference, using snapshot matrices only.\n",
    "\n",
    "For this simple test case, the latter option seems to work well. However, for\n",
    "nonlinear equations where the differential operator is nonlinear, a simple\n",
    "least squares fit will not do. Having access to differentiable ODE solver\n",
    "opens up a new world."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "kernelspec": {
   "name": "julia-1.7",
   "display_name": "Julia 1.7.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
